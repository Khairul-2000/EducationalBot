from pdf2image import convert_from_path
import re
import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain_community.chat_models import ChatOllama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from dotenv import load_dotenv
from Memory import chat_history, get_short_term_memory, format_chat_history  # Import from Memory module

# Load environment variables
load_dotenv()


api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY not found in environment variables. Please set it in .env file.")


# Replace OpenAI with ChatOpenAI
llm = ChatOpenAI(model_name="gpt-4o", temperature=0, api_key=api_key)



# 4. Better Multilingual Embeddings
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
    model_kwargs={'device': 'cpu'},  # Use 'cuda' if available
    encode_kwargs={'normalize_embeddings': True}
)


# 6. Load vector store
vectorstore = FAISS.load_local(
    "new_faiss_index_bangla_test", 
    embedding_model, 
    allow_dangerous_deserialization=True
)

# 7. Improved Prompt Template (Bengali-specific)
prompt_template = """
‡¶Ü‡¶™‡¶®‡¶æ‡¶ï‡ßá ‡¶®‡¶ø‡¶Æ‡ßç‡¶®‡¶≤‡¶ø‡¶ñ‡¶ø‡¶§ ‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡¶§‡¶•‡ßç‡¶Ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶§‡ßá ‡¶π‡¶¨‡ßá:
- ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶™‡ßç‡¶∞‡¶¶‡¶§‡ßç‡¶§ ‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡¶§‡¶•‡ßç‡¶Ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
- ‡¶Ø‡¶¶‡¶ø ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶®‡¶æ ‡¶ú‡¶æ‡¶®‡ßá‡¶®, ‡¶¨‡¶≤‡ßÅ‡¶® "‡¶Ü‡¶Æ‡¶ø ‡¶ú‡¶æ‡¶®‡¶ø ‡¶®‡¶æ"
- ‡¶â‡¶§‡ßç‡¶§‡¶∞‡¶ü‡¶ø ‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö ‡ß®-‡ß© ‡¶∂‡¶¨‡ßç‡¶¶‡ßá ‡¶¶‡¶ø‡¶®
- ‡¶ï‡ßã‡¶®‡ßã ‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶∂‡¶¨‡ßç‡¶¶, ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶¨‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø ‡¶ó‡¶†‡¶® ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶®‡¶æ
- ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶Æ‡ßÇ‡¶≤ ‡¶§‡¶•‡ßç‡¶Ø/‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ/‡¶®‡¶æ‡¶Æ ‡¶¶‡¶ø‡¶®

‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:
‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: "‡¶ï‡¶§ ‡¶∏‡¶æ‡¶≤‡ßá ‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∂‡¶ø‡¶§ ‡¶π‡¶Ø‡¶º?" 
‡¶â‡¶§‡ßç‡¶§‡¶∞: "‡ß®‡ß¶‡ß¶‡ß©"
‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: "‡¶ï‡¶§‡¶ú‡¶® ‡¶™‡¶£‡ßç‡¶°‡¶ø‡¶§ ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶®?"
‡¶â‡¶§‡ßç‡¶§‡¶∞: "‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º ‡ßß‡ß™‡ß´‡ß¶ ‡¶ú‡¶®"

‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡¶§‡¶•‡ßç‡¶Ø:
{context}

‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: {question}
‡¶â‡¶§‡ßç‡¶§‡¶∞:
"""

prompt = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

## ....................................................................
# 9. Configure RetrievalQA
qa_chain_new = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 10}),  # Get more context
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt},
    verbose=False
)

# # 10. Test with sample queries
# test_queries = [
#     " '‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ' ‡¶ó‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶¨‡¶ø‡¶Ø‡¶º‡ßá ‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶∞‡¶£ ‡¶ï‡ßÄ ‡¶õ‡¶ø‡¶≤?",
#     # "'‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ' ‡¶ó‡¶≤‡ßç‡¶™‡ßá ‡¶ó‡¶≤‡ßç‡¶™ ‡¶¨‡¶≤‡¶æ‡¶Ø‡¶º ‡¶™‡¶ü‡ßÅ ‡¶ï‡ßá ?",
# ]



# with open("test_results.txt", "w", encoding="utf-8") as file:
#     for query in test_queries:
#         if not query.strip():  # Skip empty queries
#             continue

#         # Add user query to chat history
#         chat_history.append(("User", query))

#         # Retrieve short-term memory
#         short_term_memory = get_short_term_memory(chat_history)

#         # Format the memory to pass as context
#         formatted_memory = format_chat_history(short_term_memory)

#         # Use it in the query
#         result = qa_chain_new({"query": query, "context": formatted_memory})

#         # Add system response to chat history
#         chat_history.append(("System", result['result']))

#         file.write(f"\n‚ùì ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: {query}\n")
#         file.write(f"üß† ‡¶â‡¶§‡ßç‡¶§‡¶∞: {result['result']}\n")
#         file.write("üîç ‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡¶Ö‡¶Ç‡¶∂:\n")
#         for doc in result['source_documents'][:2]:
#             file.write(f" - {doc.page_content[:100]}...\n")