from pdf2image import convert_from_path
import re
import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain_community.chat_models import ChatOllama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from dotenv import load_dotenv

# Load environment variables
load_dotenv()


api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY not found in environment variables. Please set it in .env file.")


# Replace OpenAI with ChatOpenAI
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, api_key=api_key)



# 4. Better Multilingual Embeddings
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
    model_kwargs={'device': 'cpu'},  # Use 'cuda' if available
    encode_kwargs={'normalize_embeddings': True}
)


# 6. Load vector store
vectorstore = FAISS.load_local(
    "faiss_index_bangla_test", 
    embedding_model, 
    allow_dangerous_deserialization=True
)

# 7. Improved Prompt Template (Bengali-specific)
prompt_template = """
‡¶Ü‡¶™‡¶®‡¶æ‡¶ï‡ßá ‡¶®‡¶ø‡¶Æ‡ßç‡¶®‡¶≤‡¶ø‡¶ñ‡¶ø‡¶§ ‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡¶§‡¶•‡ßç‡¶Ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶§‡ßá ‡¶π‡¶¨‡ßá:
- ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶™‡ßç‡¶∞‡¶¶‡¶§‡ßç‡¶§ ‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡¶§‡¶•‡ßç‡¶Ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
- ‡¶Ø‡¶¶‡¶ø ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶®‡¶æ ‡¶ú‡¶æ‡¶®‡ßá‡¶®, ‡¶¨‡¶≤‡ßÅ‡¶® "‡¶Ü‡¶Æ‡¶ø ‡¶ú‡¶æ‡¶®‡¶ø ‡¶®‡¶æ"
- ‡¶â‡¶§‡ßç‡¶§‡¶∞‡¶ü‡¶ø ‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡¶ø‡¶™‡ßç‡¶§ ‡¶è‡¶¨‡¶Ç ‡¶∏‡ßÅ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶∞‡¶æ‡¶ñ‡ßÅ‡¶®

‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡¶§‡¶•‡ßç‡¶Ø:
{context}

‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: {question}
‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡ßÅ‡¶≤ ‡¶â‡¶§‡ßç‡¶§‡¶∞:
"""

prompt = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

# # 8. Better LLM for Bengali
# llm = ChatOllama(
#     model="llama3:8b",  # Better for Bengali than Mistral
#     temperature=0.2,
#     num_ctx=4096  # Larger context window
# )


## ....................................................................
# 9. Configure RetrievalQA
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),  # Get more context
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt},
    verbose=False
)

# 10. Test with sample queries
test_queries = [
    "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶™‡¶ø‡¶°‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø ‡¶ï‡ßÄ?",
    "How has ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶™‡¶ø‡¶°‡¶ø‡¶Ø‡¶º‡¶æ been received since its first publication?",
    "What should readers do if they find any errors or inconsistencies in the online version of ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶™‡¶ø‡¶°‡¶ø‡¶Ø‡¶º‡¶æ?"
]

for query in test_queries:
    result = qa_chain({"query": query})
    print(f"\n‚ùì ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: {query}")
    print(f"üß† ‡¶â‡¶§‡ßç‡¶§‡¶∞: {result['result']}")
    print("üîç ‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡¶Ö‡¶Ç‡¶∂:")
    for doc in result['source_documents'][:2]:
        print(f" - {doc.page_content[:100]}...")