from difflib import SequenceMatcher
from Rag_pipeline_for_pdf import qa_chain_new



test_set = [
    # Basic Questions (Bengali)
    {
        "query": "ржмрж╛ржВрж▓рж╛ржкрж┐ржбрж┐ржпрж╝рж╛ ржХрзА?",
        "expected": "ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ ржЬрж╛рждрзАржпрж╝ ржЬрзНржЮрж╛ржиржХрзЛрж╖"
    },
    {
        "query": "ржмрж╛ржВрж▓рж╛ржкрж┐ржбрж┐ржпрж╝рж╛рждрзЗ ржХрждржЬржи ржкржгрзНржбрж┐ржд ржХрж╛ржЬ ржХрж░рзЗржЫрзЗржи?",
        "expected": "ржкрзНрж░рж╛ржпрж╝ рззрзкрзлрзж ржЬржи"
    },
    {
        "query":"How many scholars worked on the banglapedia project?",
        "expected": "ржкрзНрж░рж╛ржпрж╝ рззрзкрзлрзж ржЬржи"
    },

    # Mixed Language Questions
    {
        "query": "What is the publication year of the first edition of ржмрж╛ржВрж▓рж╛ржкрж┐ржбрж┐ржпрж╝рж╛?",
        "expected": "рзирзжрзжрзй"
    },
    {
        "query": "How many scholars worked on ржмрж╛ржВрж▓рж╛ржкрж┐ржбрж┐ржпрж╝рж╛ project?",
        "expected": "ржкрзНрж░рж╛ржпрж╝ рззрзкрзлрзж ржЬржи"
    },
    {
        "query": "ржмрж╛ржВрж▓рж╛ржкрж┐ржбрж┐ржпрж╝рж╛рж░ current edition ржХржмрзЗ published рж╣ржпрж╝рзЗржЫрзЗ?",
        "expected": "рзирзжрззрзи"
    },
    
    # Complex Questions
    {
        "query": "ржмрж╛ржВрж▓рж╛ржкрж┐ржбрж┐ржпрж╝рж╛рж░ first ржУ second edition ржПрж░ ржоржзрзНржпрзЗ ржХржд ржмржЫрж░рзЗрж░ gap?",
        "expected": "рзп ржмржЫрж░"
    },
    {
        "query": "What should readers do if they find errors in the online version of ржмрж╛ржВрж▓рж╛ржкрж┐ржбрж┐ржпрж╝рж╛?",
        "expected": "ржЖржорж╛ржжрзЗрж░ржХрзЗ ржЕржмрж╣рж┐ржд ржХрж░рзБржи"
    },
    {
        "query": "ржмрж╛ржВрж▓рж╛ржкрж┐ржбрж┐ржпрж╝рж╛ ржХрзЛржи time period cover ржХрж░рзЗ?",
        "expected": "ржкрзНрж░рж╛ржЪрзАржирждржоржХрж╛рж▓ ржерзЗржХрзЗ ржмрж░рзНрждржорж╛ржи ржкрж░рзНржпржирзНржд"
    },
    
    # Professional/Academic Questions
    {
        "query": "Who are the primary target audience of ржмрж╛ржВрж▓рж╛ржкрж┐ржбрж┐ржпрж╝рж╛ according to the text?",
        "expected": "рж╢рж┐ржХрзНрж╖ржХ, рж╢рж┐ржХрзНрж╖рж╛рж░рзНржерзА, ржЧржмрзЗрж╖ржХ, ржкрзЗрж╢рж╛ржЬрзАржмрзА ржПржмржВ рж╕рж╛ржзрж╛рж░ржг ржкрж╛ржаржХ"
    },
    {
        "query": "ржмрж╛ржВрж▓рж╛ржкрж┐ржбрж┐ржпрж╝рж╛рж░ reception ржХрзЗржоржи ржЫрж┐рж▓ after first publication?",
        "expected": "ржмрзНржпрж╛ржкржХ ржЙрзОрж╕рж╛рж╣ржмрзНржпржЮрзНржЬржХ рж╕рж╛ржбрж╝рж╛ ржкрзЗржпрж╝рзЗржЫрзЗ"
    },
    
    # Analytical Questions
    {
        "query": "ржмрж╛ржВрж▓рж╛ржкрж┐ржбрж┐ржпрж╝рж╛рж░ second edition (рзирзжрззрзи) ржПрж░ relationship with first edition (рзирзжрзжрзй) ржХрзА?",
        "expected": "ржкрж░рж┐ржмрж░рзНржзрж┐ржд ржУ рж╣рж╛рж▓ржирж╛ржЧрж╛ржж рж░рзВржк"
    },
    {
        "query": "What makes ржмрж╛ржВрж▓рж╛ржкрж┐ржбрж┐ржпрж╝рж╛ an essential companion for different professional groups?",
        "expected": "ржмрж╛ржВрж▓рж╛ржжрзЗрж╢ рж╕ржВржХрзНрж░рж╛ржирзНржд рж╕ржХрж▓ ржмрж┐рж╖ржпрж╝рзЗ рж╕рзБрж╕рзНржкрж╖рзНржЯ ржЬрзНржЮрж╛ржи"
    },
    
    # Error Detection Questions
    {
        "query": "ржЕржирж▓рж╛ржЗржи ржмрж╛ржВрж▓рж╛ржкрж┐ржбрж┐ржпрж╝рж╛ржпрж╝ ржнрзБрж▓ ржкрзЗрж▓рзЗ readers ржжрзЗрж░ ржХрзА ржХрж░рж╛ ржЙржЪрж┐ржд?",
        "expected": "ржЖржорж╛ржжрзЗрж░ржХрзЗ ржЕржмрж╣рж┐ржд ржХрж░рзБржи"
    },
    {
        "query": "How has ржмрж╛ржВрж▓рж╛ржкрж┐ржбрж┐ржпрж╝рж╛ been recognized internationally?",
        "expected": "ржЖржирзНрждрж░рзНржЬрж╛рждрж┐ржХржнрж╛ржмрзЗ ржкрзНрж░рж╢ржВрж╕рж┐ржд"
    },
    
    # Comprehensive Questions
    {
        "query": "ржмрж╛ржВрж▓рж╛ржкрж┐ржбрж┐ржпрж╝рж╛рж░ scope of coverage ржХрзА ржПржмржВ ржПрж░ impact ржХрзА рж╣ржпрж╝рзЗржЫрзЗ?",
        "expected": "ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ рж╕ржХрж▓ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржмрж┐рж╖ржпрж╝ ржПржмржВ ржЕржкрж░рж┐рж╣рж╛рж░рзНржп рж╕рж╣ржЪрж░"
    }
]


def similarity(a, b):
    return SequenceMatcher(None, a.strip().lower(), b.strip().lower()).ratio()

def evaluate_rag_system(test_set, qa_chain):
    results = []
    for item in test_set:
        query = item["query"]
        expected = item["expected"]

        response = qa_chain_new({"query": query})
        answer = response["result"]
        context = " ".join([doc.page_content for doc in response["source_documents"]])

        grounded = expected in context
        sim_score = round(similarity(expected, answer), 2)

        results.append({
            "Query": query,
            "Expected": expected,
            "Generated": answer,
            "Grounded": "тЬЕ" if grounded else "тЭМ",
            "Similarity": sim_score,
            "Context": context
        })
    
    return results


results = evaluate_rag_system(test_set, qa_chain)

for r in results:
    print("\n==========================")
    print("тЭУ ржкрзНрж░рж╢рзНржи:", r["Query"])
    print("ЁЯОп ржкрзНрж░рждрзНржпрж╛рж╢рж┐ржд:", r["Expected"])
    print("ЁЯза ржЙрждрзНрждрж░:", r["Generated"])
    print("ЁЯУЪ Grounded:", r["Grounded"])
    print("ЁЯУП Similarity:", r["Similarity"])
