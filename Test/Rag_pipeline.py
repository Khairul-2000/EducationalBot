from pdf2image import convert_from_path
import re
import os
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain_community.chat_models import ChatOllama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from dotenv import load_dotenv

# Load environment variables
load_dotenv()


api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY not found in environment variables. Please set it in .env file.")


# Replace OpenAI with ChatOpenAI
llm = ChatOpenAI(model_name="gpt-4o", temperature=0, api_key=api_key)



# 4. Better Multilingual Embeddings
# Replace OpenAI embeddings with HuggingFace e5-mistral-7b-instruct embeddings
from langchain.embeddings import HuggingFaceEmbeddings

# Initialize HuggingFace Embeddings with e5-mistral-7b-instruct
embedding_model = HuggingFaceEmbeddings(
    model_name="intfloat/e5-mistral-7b-instruct",  # Powerful multilingual embedding model
    model_kwargs={'device': 'cpu'},  # Use 'cuda' if you have GPU
    encode_kwargs={'normalize_embeddings': True}  # Normalize embeddings for better similarity search
)


# 6. Load vector store
vectorstore = FAISS.load_local(
    "new_faiss_index_bangla_test", 
    embedding_model, 
    allow_dangerous_deserialization=True
)

# 7. Improved Prompt Template (Bengali-specific)
prompt_template = """
ржЖржкржирж╛ржХрзЗ ржирж┐ржорзНржирж▓рж┐ржЦрж┐ржд ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ рждржерзНржп ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░ ржжрж┐рждрзЗ рж╣ржмрзЗ:

ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржирж┐рж░рзНржжрзЗрж╢ржирж╛:
1. ржкрзНрж░ржержорзЗ ржпрж╛ржЪрж╛ржЗ ржХрж░рзБржи ржпрзЗ ржкрзНрж░ржжрждрзНржд рждржерзНржп ржкрзНрж░рж╢рзНржирзЗрж░ рж╕рж╛ржерзЗ рж╕рж░рж╛рж╕рж░рж┐ рж╕ржорзНржкрж░рзНржХрж┐ржд ржХрж┐ ржирж╛
2. ржпржжрж┐ ржкрзНрж░ржжрждрзНржд рждржерзНржпрзЗ ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░ ржирж╛ ржерж╛ржХрзЗ, рждрж╛рж╣рж▓рзЗ "ржПржЗ рждржерзНржп ржПржЗ ржкрзНрж░рж╢рзНржирзЗрж░ рж╕рж╛ржерзЗ рж╕ржорзНржкрж░рзНржХрж┐ржд ржиржпрж╝" ржмрж▓рзБржи
3. рж╢рзБржзрзБржорж╛рждрзНрж░ ржкрзНрж░ржжрждрзНржд рждржерзНржп ржерзЗржХрзЗ рж╕рж░рж╛рж╕рж░рж┐ ржкрж╛ржУржпрж╝рж╛ ржЙрждрзНрждрж░ ржжрж┐ржи
4. ржХрзЛржирзЛ ржЕржирзБржорж╛ржи, ржмрзНржпрж╛ржЦрзНржпрж╛ ржмрж╛ ржЕрждрж┐рж░рж┐ржХрзНржд рждржерзНржп ржпрзЛржЧ ржХрж░ржмрзЗржи ржирж╛
5. ржЙрждрзНрждрж░ рж╕ржВржХрзНрж╖рж┐ржкрзНржд рж░рж╛ржЦрзБржи (рзи-рзй рж╢ржмрзНржж ржмрж╛ ржПржХржЯрж┐ ржмрж╛ржХрзНржп)

ржЙржжрж╛рж╣рж░ржг:
ржкрзНрж░рж╢рзНржи: "рж░ржмрзАржирзНржжрзНрж░ржирж╛рже ржХржд рж╕рж╛рж▓рзЗ ржирзЛржмрзЗрж▓ ржкрзЗржпрж╝рзЗржЫрзЗржи?" 
ржпржжрж┐ рждржерзНржпрзЗ ржЖржЫрзЗ: "рззрзпрззрзй"
ржпржжрж┐ рждржерзНржпрзЗ ржирзЗржЗ: "ржПржЗ рждржерзНржп ржПржЗ ржкрзНрж░рж╢рзНржирзЗрж░ рж╕рж╛ржерзЗ рж╕ржорзНржкрж░рзНржХрж┐ржд ржиржпрж╝"

ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ рждржерзНржп:
{context}

ржкрзНрж░рж╢рзНржи: {question}
ржЙрждрзНрждрж░:
"""

prompt = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

# # 8. Better LLM for Bengali
# llm = ChatOllama(
#     model="llama3:8b",  # Better for Bengali than Mistral
#     temperature=0.2,
#     num_ctx=4096  # Larger context window
# )


## ....................................................................
# 9. Configure RetrievalQA
qa_chain_new = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(
        search_kwargs={
            "k": 4,
            "score_threshold": 0.5  # Specify the score threshold as a float
        },
        search_type="similarity_score_threshold"
    ),
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt},
    verbose=False
)

# 10. Test with sample queries
test_queries = [
    "ржЕржкрж░рж┐ржЪрж┐рждрж╛ ржЧрж▓рзНржкрзЗрж░ рж▓рзЗржЦржХ ржХрзЗ?",
    "ржЕржкрж░рж┐ржЪрж┐рждрж╛ ржЧрж▓рзНржкрзЗ ржЕржирзБржкржо ржХрзЛржерж╛ржпрж╝ ржЪрж╛ржХрж░рж┐ ржХрж░рждрзЗржи?",
    "What is the name of the female protagonist in ржЕржкрж░рж┐ржЪрж┐рждрж╛?"
]

for query in test_queries:
    result = qa_chain_new({"query": query})
    print(f"\nтЭУ ржкрзНрж░рж╢рзНржи: {query}")
    print(f"ЁЯза ржЙрждрзНрждрж░: {result['result']}")
    print("ЁЯФН ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ ржЕржВрж╢:")
    for doc in result['source_documents'][:2]:
        print(f" - {doc.page_content[:100]}...")