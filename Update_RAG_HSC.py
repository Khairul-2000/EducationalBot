import os
import shutil
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from dotenv import load_dotenv

def update_rag_for_hsc():
    """
    Update the RAG pipeline to use HSC knowledge base
    """
    load_dotenv()
    
    print("ЁЯФД Updating RAG Pipeline for HSC Content")
    print("=" * 50)
    
    # Check available knowledge bases
    available_kbs = []
    kb_options = [
        "faiss_index_bangla_test",
        "faiss_index_hsc_fast", 
        "faiss_index_hsc_manual"
    ]
    
    for kb in kb_options:
        if os.path.exists(kb):
            available_kbs.append(kb)
            print(f"тЬЕ Found: {kb}")
        else:
            print(f"тЭМ Missing: {kb}")
    
    if not available_kbs:
        print("тЭМ No knowledge bases found!")
        return None
    
    # Use the best available knowledge base
    if "faiss_index_hsc_manual" in available_kbs:
        selected_kb = "faiss_index_hsc_manual"
        print(f"ЁЯОп Using manual HSC knowledge base")
    elif "faiss_index_bangla_test" in available_kbs:
        selected_kb = "faiss_index_bangla_test" 
        print(f"ЁЯОп Using existing Bengali knowledge base")
    else:
        selected_kb = available_kbs[0]
        print(f"ЁЯОп Using: {selected_kb}")
    
    # Load embedding model
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    # Load vector store
    try:
        vectorstore = FAISS.load_local(
            selected_kb, 
            embedding_model, 
            allow_dangerous_deserialization=True
        )
        print(f"тЬЕ Loaded knowledge base: {selected_kb}")
    except Exception as e:
        print(f"тЭМ Error loading {selected_kb}: {e}")
        return None
    
    # Setup LLM
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("тЭМ OPENAI_API_KEY not found!")
        return None
    
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, api_key=api_key)
    
    # Create improved prompt for HSC content
    prompt_template = """
ржЖржкржирж┐ ржПржХржЬржи ржмрж╛ржВрж▓рж╛ рж╕рж╛рж╣рж┐рждрзНржп ржУ ржмрзНржпрж╛ржХрж░ржгрзЗрж░ рж╢рж┐ржХрзНрж╖ржХред ржирж┐ржорзНржирж▓рж┐ржЦрж┐ржд ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ рждржерзНржп ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░ ржжрж┐ржи:

ржирж┐ржпрж╝ржорж╛ржмрж▓рзА:
- рж╢рзБржзрзБржорж╛рждрзНрж░ ржкрзНрж░ржжрждрзНржд ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ рждржерзНржп ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи
- ржЙрждрзНрждрж░ рж╕ржВржХрзНрж╖рж┐ржкрзНржд ржУ рж╕рзБржирж┐рж░рзНржжрж┐рж╖рзНржЯ рж░рж╛ржЦрзБржи (рж╕рж░рзНржмрзЛржЪрзНржЪ рзи-рзй рж╢ржмрзНржж)
- ржпржжрж┐ ржЙрждрзНрждрж░ ржирж╛ ржЬрж╛ржирзЗржи, ржмрж▓рзБржи "рждржерзНржп ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ржирж┐"
- ржЕрждрж┐рж░рж┐ржХрзНржд ржмрзНржпрж╛ржЦрзНржпрж╛ ржжрзЗржмрзЗржи ржирж╛

ржЙржжрж╛рж╣рж░ржг:
ржкрзНрж░рж╢рзНржи: "рж░ржмрзАржирзНржжрзНрж░ржирж╛рже ржХржмрзЗ ржЬржирзНржоржЧрзНрж░рж╣ржг ржХрж░рзЗржи?"
ржЙрждрзНрждрж░: "рзн ржорзЗ, рззрзорзмрзз"

ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ рждржерзНржп:
{context}

ржкрзНрж░рж╢рзНржи: {question}
ржЙрждрзНрждрж░:
"""

    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )
    
    # Create QA chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt},
        verbose=False
    )
    
    print("тЬЕ QA Chain created successfully")
    
    # Test with HSC-related queries
    test_queries = [
        "рж░ржмрзАржирзНржжрзНрж░ржирж╛рже ржарж╛ржХрзБрж░ ржХржмрзЗ ржЬржирзНржоржЧрзНрж░рж╣ржг ржХрж░рзЗржи?",
        "ржЧрзАрждрж╛ржЮрзНржЬрж▓рж┐ ржХрж╛ржмрзНржпржЧрзНрж░ржирзНржерзЗрж░ ржЬржирзНржп ржХрзА ржкрзБрж░рж╕рзНржХрж╛рж░ ржкрзЗржпрж╝рзЗржЫрж┐рж▓рзЗржи?",
        "ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ржпрж╝ ржХржпрж╝ржЯрж┐ рж╕рзНржмрж░ржзрзНржмржирж┐ ржЖржЫрзЗ?",
        "рж░ржмрзАржирзНржжрзНрж░ржирж╛ржерзЗрж░ ржмрж┐ржЦрзНржпрж╛ржд ржЙржкржирзНржпрж╛рж╕рзЗрж░ ржирж╛ржо ржХрзА?"
    ]
    
    print("\nЁЯзк Testing QA System:")
    print("-" * 30)
    
    for query in test_queries:
        try:
            result = qa_chain({"query": query})
            print(f"тЭУ ржкрзНрж░рж╢рзНржи: {query}")
            print(f"ЁЯза ржЙрждрзНрждрж░: {result['result']}")
            print(f"ЁЯУЪ рж╕рзЛрж░рзНрж╕: {len(result['source_documents'])} documents")
            print("-" * 30)
        except Exception as e:
            print(f"тЭМ Error with query '{query}': {e}")
    
    # Save updated configuration
    config_info = f"""
# Updated RAG Configuration
Knowledge Base: {selected_kb}
Last Updated: {os.path.basename(__file__)}
Status: Ready for HSC queries
"""
    
    with open("rag_config.txt", "w", encoding="utf-8") as f:
        f.write(config_info)
    
    print(f"\nЁЯОЙ RAG Pipeline updated successfully!")
    print(f"ЁЯУБ Using knowledge base: {selected_kb}")
    print(f"ЁЯТб Ready for HSC Bengali literature and grammar questions")
    
    return qa_chain

if __name__ == "__main__":
    qa_chain = update_rag_for_hsc()
